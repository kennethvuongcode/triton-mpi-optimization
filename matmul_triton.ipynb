{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "E-mNhUjQuxNM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "import triton.testing as testing\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "N9lmLw8cuxNN"
      },
      "outputs": [],
      "source": [
        "def is_cuda():\n",
        "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eUMlpjFJuxNO"
      },
      "outputs": [],
      "source": [
        "def is_hip_mi200():\n",
        "    target = triton.runtime.driver.active.get_current_target()\n",
        "    return target.backend == 'hip' and target.arch == 'gfx90a'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lBNGYaejuxNO"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PA2 Part 2: MatMul+Relu+Add Fused Optimization.\n",
        "The kernel uses several optimization techniques:\n",
        "\n",
        "  1. Shared memory tiling.\n",
        "  2. Register tiling.\n",
        "  3. Cooperative fetching.\n",
        "  4. Operator Fusion\n",
        "  5. Write cache / epilogue fusion.\n",
        "\n",
        "Fill in the missing parts (marked with TODO).\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Tiling parameters - You will need to change these to achieve better results.\n",
        "# -----------------------------------------------------------------------------\n",
        "BLOCK_M = 128  # Tile size in the M dimension.\n",
        "BLOCK_N = 256 # Tile size in the N dimension.\n",
        "BLOCK_K = 16 # Tile size in the K dimension.\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Triton Kernel: Matrix Multiplication + ReLU + Add\n",
        "#\n",
        "# The kernel uses:\n",
        "#   Step 1: Tile assignment (each kernel computes a tile of C)\n",
        "#   Step 2: Shared memory tiling + Cooperative Fetching: Load tiles of A and B.\n",
        "#   Step 3: Register tiling: Use a register accumulator.\n",
        "#   Step 4: Add and ReLU fusion\n",
        "#   Step 5: Write cache/Epilogue: Write the final tile back to global memory.\n",
        "# -----------------------------------------------------------------------------\n",
        "@triton.jit\n",
        "def matmul_add_relu_kernel_fp16(\n",
        "    a_ptr, b_ptr, c_ptr, d_ptr,\n",
        "    M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n",
        "    stride_am: tl.constexpr, stride_ak: tl.constexpr,\n",
        "    stride_bk: tl.constexpr, stride_bn: tl.constexpr,\n",
        "    stride_cm: tl.constexpr, stride_cn: tl.constexpr,\n",
        "    stride_dm: tl.constexpr, stride_dn: tl.constexpr,\n",
        "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
        "):\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 1: Tile: Assignment\n",
        "    #\n",
        "    # Each kernel instance is mapped to a tile in the output matrix C.\n",
        "    # Compute the starting indices (m_start, n_start) for this tile.\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Compute the tile indices using program_id(0) for M and program_id(1) for N.\n",
        "    pid_m = tl.program_id(0)\n",
        "    pid_n = tl.program_id(1)\n",
        "\n",
        "    m_start = pid_m * BLOCK_M\n",
        "    n_start = pid_n * BLOCK_N\n",
        "\n",
        "    offset_m = m_start + tl.arange(0, BLOCK_M)\n",
        "    offset_n = n_start + tl.arange(0, BLOCK_N)\n",
        "    k_arange = tl.arange(0, BLOCK_K)\n",
        "\n",
        "    offset_m_reshape = offset_m[:, None]\n",
        "    offset_n_reshape = offset_n[None, :]\n",
        "\n",
        "    offset_am = offset_m_reshape * stride_am\n",
        "    offset_bn = offset_n_reshape * stride_bn\n",
        "\n",
        "    mask_m = offset_m_reshape < M\n",
        "    mask_n = offset_n_reshape < N\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 2: Register Tiling\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Initialize the accumulator \"acc\" with zeros (dtype: float16).\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype = tl.float16)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 3: Shared Memory Tiling & Cooperative Fetching.\n",
        "    # Compute pointers to the sub-tiles of A and B that are needed to compute\n",
        "    # the current C tile. The offsets here serve to load BLOCK_SIZE_M x BLOCK_SIZE_K\n",
        "    # and BLOCK_SIZE_K x BLOCK_SIZE_N blocks from A and B respectively.\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Finish code below\n",
        "    for k in range(0, K, BLOCK_K):\n",
        "        offset_k = (k + k_arange)\n",
        "        offset_k_reshape1 = offset_k[None, :]\n",
        "        offset_k_reshape2 = offset_k[:, None]\n",
        "\n",
        "        offset_ak = offset_k_reshape1 * stride_ak\n",
        "        offset_bk = offset_k_reshape2 * stride_bk\n",
        "\n",
        "        mask_ak = offset_k_reshape1 < K\n",
        "        mask_bk = offset_k_reshape2 < K\n",
        "\n",
        "        mask_a = mask_m & mask_ak\n",
        "        mask_b = mask_bk & mask_n\n",
        "\n",
        "        tile_a = tl.load(a_ptr + offset_am + offset_ak, mask = mask_m, other = 0.0)\n",
        "        tile_b = tl.load(b_ptr + offset_bk + offset_bn, mask = mask_b, other = 0.0)\n",
        "\n",
        "        acc += tl.dot(tile_a, tile_b, out_dtype = tl.float16)\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 4: Apply ReLU and Add C to the accumulator\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Finish code below\n",
        "\n",
        "    offset_cm = offset_m_reshape * stride_cm\n",
        "    offset_cn = offset_n_reshape * stride_cn\n",
        "    offset_c = offset_cm + offset_cn\n",
        "\n",
        "    mask_cm = offset_m_reshape < M\n",
        "    mask_cn = offset_n_reshape < N\n",
        "    mask_c = mask_cm & mask_cn\n",
        "\n",
        "    tile_c = tl.load(c_ptr + offset_c, mask = mask_c, other = 0.0)\n",
        "    acc += tile_c\n",
        "    acc = tl.maximum(acc, 0.0)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Step 5: Write Cache / Epilogue Fusion: Write the computed tile to D.\n",
        "    # -------------------------------------------------------------------------\n",
        "    # TODO: Finish code below\n",
        "    offset_dm = offset_m_reshape * stride_dm\n",
        "    offset_dn = offset_n_reshape * stride_dn\n",
        "    offset_d = offset_dm + offset_dn\n",
        "\n",
        "    mask_dm = offset_m_reshape < M\n",
        "    mask_dn = offset_n_reshape < N\n",
        "    mask_d = mask_dm & mask_dn\n",
        "\n",
        "    tl.store(d_ptr + offset_d, acc, mask = mask_d)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "u16sz-IUuxNP"
      },
      "outputs": [],
      "source": [
        "def matmul_add_relu_fp16(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes Output = ReLU(A @ B + C) using fp16 precision for maximum throughput.\n",
        "    \"\"\"\n",
        "    M, K = a.shape\n",
        "    K2, N = b.shape\n",
        "    assert K == K2, \"Incompatible dimensions\"\n",
        "\n",
        "    d = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
        "    # Create launch grid\n",
        "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n",
        "\n",
        "    matmul_add_relu_kernel_fp16[grid](\n",
        "        a, b, c, d,\n",
        "        M, N, K,\n",
        "        a.stride(0), a.stride(1),\n",
        "        b.stride(0), b.stride(1),\n",
        "        c.stride(0), c.stride(1),\n",
        "        d.stride(0), d.stride(1),\n",
        "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K\n",
        "    )\n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AJ7LlTPawPqB"
      },
      "outputs": [],
      "source": [
        "# Reference implementation using PyTorch\n",
        "def reference_matmul_add_relu(A, B, C):\n",
        "    result = torch.matmul(A, B).add(C).relu_()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "B4J5ZBpOuxNP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4ac394db-374e-4dbe-d2ae-cf25eb5d6fa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "triton_output_with_fp16_inputs=tensor([[ 0.0000,  6.1250,  0.0000,  ..., 10.0625,  0.0000,  0.0000],\n",
            "        [ 7.9102, 15.6328, 26.6094,  ..., 11.4609,  5.3750, 18.6250],\n",
            "        [ 2.7246,  0.0000,  0.0000,  ...,  0.0000, 26.0781,  0.0000],\n",
            "        ...,\n",
            "        [ 0.4448, 75.1875,  0.0000,  ..., 26.2812,  0.0000,  0.0000],\n",
            "        [ 6.9492,  1.1230,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [27.6094, 26.9531, 22.9219,  ..., 13.5391,  6.0508, 21.6250]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "torch_output_with_fp16_inputs=tensor([[ 0.0000,  6.1289,  0.0000,  ..., 10.0391,  0.0000,  0.0000],\n",
            "        [ 7.9102, 15.6328, 26.6250,  ..., 11.4531,  5.3945, 18.6562],\n",
            "        [ 2.7266,  0.0000,  0.0000,  ...,  0.0000, 26.1250,  0.0000],\n",
            "        ...,\n",
            "        [ 0.4316, 75.2500,  0.0000,  ..., 26.2812,  0.0000,  0.0000],\n",
            "        [ 6.9570,  1.1260,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [27.6406, 26.9531, 22.9375,  ..., 13.5625,  6.0391, 21.6406]],\n",
            "       device='cuda:0', dtype=torch.float16)\n",
            "✅ Triton and Torch match\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Accuracy Tests\n",
        "# -----------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(0)\n",
        "    a = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    b = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    c = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
        "    triton_output = matmul_add_relu_fp16(a, b, c)\n",
        "    torch_output = reference_matmul_add_relu(a, b, c)\n",
        "    print(f\"triton_output_with_fp16_inputs={triton_output}\")\n",
        "    print(f\"torch_output_with_fp16_inputs={torch_output}\")\n",
        "    rtol = 1e-2 if is_hip_mi200() else 0.032\n",
        "    if torch.allclose(triton_output, torch_output, atol=0.15, rtol=rtol):\n",
        "        print(\"✅ Triton and Torch match\")\n",
        "    else:\n",
        "        diff = triton_output - torch_output\n",
        "        abs_diff = torch.abs(diff)\n",
        "        max_abs_diff = torch.max(abs_diff)\n",
        "        print(f\"❌ Triton and Torch differ: {max_abs_diff=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kj_dGOlazQJY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e18fa3bb-3c86-44cc-c8d3-8cdda513eddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 0.76 ms\n",
            "PyTorch implementation: 1.08 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 1.42x\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# Performance Benchmark\n",
        "# IMPORTANT: DO NOT CHANGE THIS CODE.\n",
        "# THIS IS THE EXACT CODE THAT WILL BE USED TO GRADE YOUR IMPLEMENTATION.\n",
        "# ANY CHANGES TO THIS CODE (INCLUDING DIMENSIONS, REPEATS, etc.)\n",
        "# WILL CAUSE YOU TO HAVE DIFFERENT SPEEDUP RESULTS.\n",
        "# -----------------------------------------------------------------------------\n",
        "M = 2048\n",
        "K = 2048\n",
        "N = 2048\n",
        "\n",
        "# KEEP THESE MATRICES IN FP16. FP32 WILL NOT PROVIDE ACCURATE RESULTS\n",
        "A = torch.randn((M, K), device=\"cuda\", dtype=torch.float16)\n",
        "B = torch.randn((K, N), device=\"cuda\", dtype=torch.float16)\n",
        "C = torch.randn((M, N), device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "# warmup\n",
        "_ = matmul_add_relu_fp16(A, B, C)\n",
        "_ = reference_matmul_add_relu(A, B, C)\n",
        "\n",
        "REPEATS = 5000\n",
        "\n",
        "# time your implementation\n",
        "print(\"Triton implementation\")\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(REPEATS):\n",
        "    _ = matmul_add_relu_fp16(A, B, C)\n",
        "torch.cuda.synchronize()\n",
        "triton_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "# time pytorch\n",
        "print(\"PyTorch implementation\")\n",
        "torch.cuda.synchronize()\n",
        "start = time.perf_counter()\n",
        "for _ in range(REPEATS):\n",
        "    _ = reference_matmul_add_relu(A, B, C)\n",
        "torch.cuda.synchronize()\n",
        "torch_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "print(f\"Performance comparison for matrix multiplication ({M}x{K} @ {K}x{N}):\")\n",
        "print(f\"Triton implementation: {triton_time*1000:.2f} ms\")\n",
        "print(f\"PyTorch implementation: {torch_time*1000:.2f} ms\")\n",
        "\n",
        "print(f\"\\nSpeedup of Triton vs PyTorch: {torch_time/triton_time:.2f}x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "K9Hdpxic0tq6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "cd8b80a2-140a-4894-d1cd-22bb073bf1a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 32, block_n = 32, block_k = 16\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 4.65 ms\n",
            "PyTorch implementation: 1.05 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.23x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 32, block_n = 32, block_k = 32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 4.73 ms\n",
            "PyTorch implementation: 1.12 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.24x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 32, block_n = 32, block_k = 64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 4.02 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.26x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 32, block_n = 32, block_k = 128\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 3.43 ms\n",
            "PyTorch implementation: 1.05 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.31x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 32, block_n = 64, block_k = 16\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 3.02 ms\n",
            "PyTorch implementation: 1.07 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.36x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 32, block_n = 64, block_k = 32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 2.96 ms\n",
            "PyTorch implementation: 1.08 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.36x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 32, block_n = 64, block_k = 64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 2.60 ms\n",
            "PyTorch implementation: 1.07 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.41x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 32, block_n = 64, block_k = 128\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 2.13 ms\n",
            "PyTorch implementation: 1.07 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.50x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 32, block_n = 128, block_k = 16\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 2.05 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.52x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 32, block_n = 128, block_k = 32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 2.02 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.52x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 32, block_n = 128, block_k = 64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.49 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.71x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 32, block_n = 128, block_k = 128\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.97 ms\n",
            "PyTorch implementation: 1.07 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.54x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 32, block_n = 256, block_k = 16\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.50 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.71x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 32, block_n = 256, block_k = 32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.35 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.79x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 32, block_n = 256, block_k = 64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.76 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.60x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 32, block_n = 256, block_k = 128\n",
            "Triton implementation\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "out of resources for block_m = 32, block_n = 256, block_k = 128\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 64, block_n = 32, block_k = 16\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 4.04 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.26x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 64, block_n = 32, block_k = 32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 3.99 ms\n",
            "PyTorch implementation: 1.08 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.27x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 64, block_n = 32, block_k = 64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 3.21 ms\n",
            "PyTorch implementation: 1.08 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.34x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 64, block_n = 32, block_k = 128\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 2.96 ms\n",
            "PyTorch implementation: 1.07 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.36x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 64, block_n = 64, block_k = 16\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 2.34 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.45x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 64, block_n = 64, block_k = 32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 2.22 ms\n",
            "PyTorch implementation: 1.07 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.48x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 64, block_n = 64, block_k = 64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.91 ms\n",
            "PyTorch implementation: 1.07 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.56x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 64, block_n = 64, block_k = 128\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.72 ms\n",
            "PyTorch implementation: 1.07 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.62x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 64, block_n = 128, block_k = 16\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.51 ms\n",
            "PyTorch implementation: 1.07 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.71x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 64, block_n = 128, block_k = 32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.42 ms\n",
            "PyTorch implementation: 1.07 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.75x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 64, block_n = 128, block_k = 64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.23 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.87x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 64, block_n = 128, block_k = 128\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.45 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.73x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 64, block_n = 256, block_k = 16\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.02 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 1.04x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 64, block_n = 256, block_k = 32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.11 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.96x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 64, block_n = 256, block_k = 64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.24 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.85x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 64, block_n = 256, block_k = 128\n",
            "Triton implementation\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "out of resources for block_m = 64, block_n = 256, block_k = 128\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 128, block_n = 32, block_k = 16\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 2.93 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.36x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 128, block_n = 32, block_k = 32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 3.02 ms\n",
            "PyTorch implementation: 1.08 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.36x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 128, block_n = 32, block_k = 64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 2.22 ms\n",
            "PyTorch implementation: 1.08 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.49x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 128, block_n = 32, block_k = 128\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 2.01 ms\n",
            "PyTorch implementation: 1.07 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.53x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 128, block_n = 64, block_k = 16\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.52 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.70x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 128, block_n = 64, block_k = 32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.58 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.67x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 128, block_n = 64, block_k = 64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.34 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.79x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 128, block_n = 64, block_k = 128\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.41 ms\n",
            "PyTorch implementation: 1.07 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.76x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 128, block_n = 128, block_k = 16\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 0.93 ms\n",
            "PyTorch implementation: 1.07 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 1.15x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 128, block_n = 128, block_k = 32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.07 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.99x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 128, block_n = 128, block_k = 64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 0.91 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 1.17x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 128, block_n = 128, block_k = 128\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 0.96 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 1.11x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 128, block_n = 256, block_k = 16\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 0.76 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 1.39x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 128, block_n = 256, block_k = 32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 0.81 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 1.31x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 128, block_n = 256, block_k = 64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 0.94 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 1.12x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 128, block_n = 256, block_k = 128\n",
            "Triton implementation\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "out of resources for block_m = 128, block_n = 256, block_k = 128\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 256, block_n = 32, block_k = 16\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 2.12 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.50x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 256, block_n = 32, block_k = 32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.89 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.56x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 256, block_n = 32, block_k = 64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.75 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.61x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 256, block_n = 32, block_k = 128\n",
            "Triton implementation\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "out of resources for block_m = 256, block_n = 32, block_k = 128\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 256, block_n = 64, block_k = 16\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.21 ms\n",
            "PyTorch implementation: 1.07 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.88x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 256, block_n = 64, block_k = 32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.32 ms\n",
            "PyTorch implementation: 1.07 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.81x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 256, block_n = 64, block_k = 64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.30 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.82x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 256, block_n = 64, block_k = 128\n",
            "Triton implementation\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "out of resources for block_m = 256, block_n = 64, block_k = 128\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 256, block_n = 128, block_k = 16\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 0.80 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 1.34x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 256, block_n = 128, block_k = 32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 0.82 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 1.29x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 256, block_n = 128, block_k = 64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 1.02 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 1.04x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 256, block_n = 128, block_k = 128\n",
            "Triton implementation\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "out of resources for block_m = 256, block_n = 128, block_k = 128\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 256, block_n = 256, block_k = 16\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 2.00 ms\n",
            "PyTorch implementation: 1.05 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.53x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 256, block_n = 256, block_k = 32\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 3.87 ms\n",
            "PyTorch implementation: 1.06 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.27x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 256, block_n = 256, block_k = 64\n",
            "Triton implementation\n",
            "PyTorch implementation\n",
            "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
            "Triton implementation: 19.75 ms\n",
            "PyTorch implementation: 1.07 ms\n",
            "\n",
            "Speedup of Triton vs PyTorch: 0.05x\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Testing block_m = 256, block_n = 256, block_k = 128\n",
            "Triton implementation\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "out of resources for block_m = 256, block_n = 256, block_k = 128\n",
            "\n",
            "Best speedup: 1.39x\n",
            "Best BLOCK_M: 128\n",
            "Best BLOCK_N: 256\n",
            "Best BLOCK_K: 16\n"
          ]
        }
      ],
      "source": [
        "# Write your grid search here.\n",
        "block_m_vals = [32, 64, 128, 256]\n",
        "block_n_vals = [32, 64, 128, 256]\n",
        "block_k_vals = [16, 32, 64, 128]\n",
        "best_speedup = 0\n",
        "best_block_vals = None\n",
        "for block_m in block_m_vals:\n",
        "    for block_n in block_n_vals:\n",
        "        for block_k in block_k_vals:\n",
        "          try:\n",
        "            print(\"\\n----------------------------------------------------------------------\")\n",
        "            print(f\"Testing block_m = {block_m}, block_n = {block_n}, block_k = {block_k}\")\n",
        "            BLOCK_M = block_m\n",
        "            BLOCK_N = block_n\n",
        "            BLOCK_K = block_k\n",
        "            print(\"Triton implementation\")\n",
        "            torch.cuda.synchronize()\n",
        "            start = time.perf_counter()\n",
        "            for _ in range(REPEATS):\n",
        "                _ = matmul_add_relu_fp16(A, B, C)\n",
        "            torch.cuda.synchronize()\n",
        "            triton_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "            # time pytorch\n",
        "            print(\"PyTorch implementation\")\n",
        "            torch.cuda.synchronize()\n",
        "            start = time.perf_counter()\n",
        "            for _ in range(REPEATS):\n",
        "                _ = reference_matmul_add_relu(A, B, C)\n",
        "            torch.cuda.synchronize()\n",
        "            torch_time = (time.perf_counter() - start) / REPEATS\n",
        "\n",
        "            print(f\"Performance comparison for matrix multiplication ({M}x{K} @ {K}x{N}):\")\n",
        "            print(f\"Triton implementation: {triton_time*1000:.2f} ms\")\n",
        "            print(f\"PyTorch implementation: {torch_time*1000:.2f} ms\")\n",
        "\n",
        "            speedup = torch_time/triton_time\n",
        "            if speedup > best_speedup:\n",
        "              best_speedup = speedup\n",
        "              best_block_vals = [block_m, block_n, block_k]\n",
        "            print(f\"\\nSpeedup of Triton vs PyTorch: {speedup:.2f}x\")\n",
        "\n",
        "          except triton.OutOfResources:\n",
        "            print(\"\\n----------------------------------------------------------------------\")\n",
        "            print(f\"out of resources for block_m = {block_m}, block_n = {block_n}, block_k = {block_k}\")\n",
        "\n",
        "\n",
        "print(f\"\\nBest speedup: {best_speedup:.2f}x\")\n",
        "print(f\"Best BLOCK_M: {best_block_vals[0]}\")\n",
        "print(f\"Best BLOCK_N: {best_block_vals[1]}\")\n",
        "print(f\"Best BLOCK_K: {best_block_vals[2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t3Vb9LUJ4RON"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KERTNa79Oo2M"
      },
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}