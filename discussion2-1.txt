My myAllreduce and myAlltoall functions are both slower than the MPI implementation, with my myAllreduce being about twice as slow and my myAlltoall being about three times as slow. 
This is probably due to the method of which I communicate between ranks. For myAllreduce, all processes first send data to rank 0, which then broadcasts the result to all other processes. Rank 0 must therefore handle N-1 receives and N-1 sends, making it linear in time complexity. The MPI implementation uses a binary tree-based reduction which is O(logN) or ring based method, both being faster than linear time complexity.
My myAlltoall implementation is a pairwise send-receive approach, which O(N^2) total messages. MPI likely uses a ring algorithm, which distributes communication across processes and is faster.